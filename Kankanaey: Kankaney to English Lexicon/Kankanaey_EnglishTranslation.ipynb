{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ASVRDPwQwmM",
        "outputId": "672e0141-78d5-4a1e-d8fa-dc55bf3df25f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import llibraries and frameworks\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    EarlyStoppingCallback\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKasFYJ9Q6Nb",
        "outputId": "2e2d6f73-60d6-49e0-e4b5-c745db673247"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lori-X42QNOa",
        "outputId": "0722b978-1c05-44a1-8cc3-287e163cd4f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example input (raw): Entako ed bilig\n",
            "Example target (raw): Let us go to the forest.\n",
            "Translation (model): Let's go to the forest\n"
          ]
        }
      ],
      "source": [
        "# inference example on how to translate a Kankanaey word/phrase/sentence using the trained model\n",
        "\n",
        "## load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/nllb_kke_model\")\n",
        "\n",
        "## load and configure the model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    \"/content/drive/MyDrive/nllb_kke_model\",\n",
        "    # low_cpu_mem_usage=True,\n",
        "    # offload_folder=\"offload\",\n",
        "    # offload_state_dict=True,\n",
        "    # torch_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "## translation function\n",
        "def translate_text(text, max_length=128, num_beams=4):\n",
        "    # tokenize the source text and set src_lang via tokenizer (surrogate)\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\", truncation=True, padding=True, max_length=200)\n",
        "    # if torch.cuda.is_available():\n",
        "    #     inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "    #     model.to(\"cuda\")\n",
        "    # Ensure model generates in target lang by forcing bos token id (if set earlier)\n",
        "    gen = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        num_beams=num_beams,\n",
        "        early_stopping=True,\n",
        "        forced_bos_token_id=getattr(model.config, \"forced_bos_token_id\", None)\n",
        "    )\n",
        "    out = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
        "    return out[0].strip()\n",
        "\n",
        "example_src = \"Entako ed bilig\"\n",
        "exam_trans_tgt = \"Let us go to the forest.\"\n",
        "print(\"Example input (raw):\", example_src)\n",
        "print(\"Example target (raw):\", exam_trans_tgt)\n",
        "print(\"Translation (model):\", translate_text(example_src))"
      ]
    }
  ]
}